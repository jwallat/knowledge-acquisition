# Data dir
data_dir: '/home/wallat/knowledge-probing/data/'

# Output base dir
# output_base_dir: /home/wallat/knowledge-probing/data/outputs/msmarco/

# Model dirs
# pre_trained_model_dir: './models/pre-trained-bert/'
qa_model_dir: '/home/wallat/knowledge-probing/data/models/qa-bert-squad2-cased/'


# Training
wiki2_train_data_file: "/home/wallat/knowledge-probing/data/training_data/wikitext-2-raw/wiki.train.raw"
wiki2_eval_data_file: "/home/wallat/knowledge-probing/data/training_data/wikitext-2-raw/wiki.valid.raw"
wiki2_test_data_file: "/home/wallat/knowledge-probing/data/training_data/wikitext-2-raw/wiki.test.raw"
 
wiki103_train_data_file: "/home/wallat/knowledge-probing/data/training_data/wikitext-103-raw/wiki.train.raw"
wiki103_eval_data_file: "/home/wallat/knowledge-probing/data/training_data/wikitext-103-raw/wiki.valid.raw"
wiki103_test_data_file: "/home/wallat/knowledge-probing/data/training_data/wikitext-103-raw/wiki.test.raw"


mlm_probability: 0.15 # Ratio of tokens to mask for masked language modeling loss
val_check_interval: 563

per_gpu_train_batch_size: 8
per_gpu_eval_batch_size: 8
per_gpu_test_batch_size: 8

max_steps: 200000
learning_rate: 5e-5

row_log_interval: 50 
log_save_interval: 500

progress_bar_refresh_rate: 500


################### Probing
probing_data_dir: '/home/wallat/knowledge-probing/data/probing_data/'
probing_batch_size: 16
precision_at_k: 100
