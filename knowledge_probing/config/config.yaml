# Data dir
data_dir: 'data/'

# Output base dir
output_base_dir: data/outputs/

# Model dirs
# pre_trained_model_dir: './models/pre-trained-bert/'
qa_model_dir: 'data/models/qa-bert/'


# Training
wiki2_train_data_file: "data/training_data/wikitext-2-raw/wiki.train.raw"
wiki2_eval_data_file: "data/training_data/wikitext-2-raw/wiki.valid.raw"
wiki2_test_data_file: "data/training_data/wikitext-2-raw/wiki.test.raw"
 
wiki103_train_data_file: "data/training_data/wikitext-103-raw/wiki.train.raw"
wiki103_eval_data_file: "data/training_data/wikitext-103-raw/wiki.valid.raw"
wiki103_test_data_file: "data/training_data/wikitext-103-raw/wiki.test.raw"


mlm_probability: 0.15 # Ratio of tokens to mask for masked language modeling loss
val_check_interval: 50

per_gpu_train_batch_size: 8
per_gpu_eval_batch_size: 8
per_gpu_test_batch_size: 8

max_steps: 200000
learning_rate: 5e-5

row_log_interval: 50 
log_save_interval: 500

progress_bar_refresh_rate: 500


################### Probing
probing_data_dir: 'data/probing_data/'
probing_batch_size: 16
precision_at_k: 100
