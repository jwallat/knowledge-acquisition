# Data dir
data_dir: 'data/'

# Model dirs
# pre_trained_model_dir: './models/pre-trained-bert/'
qa_model_dir: 'data/qa-bert/'


# Training

wiki2_train_data_file: "wikitext-2-raw/wiki.raw"
wiki2_eval_data_file: "wikitext-2-raw/wiki.valid.raw"
wiki2_test_data_file: "wikitext-2-raw/wiki.test.raw"
 
wiki103_train_data_file: "wikitext-103-raw/wiki.raw"
wiki103_eval_data_file: "wikitext-103-raw/wiki.valid.raw"
wiki103_test_data_file: "wikitext-103-raw/wiki.test.raw"


mlm_probability: 0.15 # Ratio of tokens to mask for masked language modeling loss
val_check_interval: 563

per_gpu_train_batch_size: 8
per_gpu_eval_batch_size: 8
per_gpu_test_batch_size: 8

max_steps: 200000
learning_rate: 5e-5


################### Probing
probing_data_dir: 'data/probing_data/'
probing_batch_size: 16
